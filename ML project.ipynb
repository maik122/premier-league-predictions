{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Football match result prediction</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions for evaluation:\n",
    "- Class report\n",
    "- Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fancy_Class_Report(y_true, y_pred):\n",
    "    report = classification_report(y_true,y_pred, output_dict=True)\n",
    "\n",
    "    classes = list(report.keys())[:-3]\n",
    "    metrics = ['precision', 'recall', 'f1-score', 'support']\n",
    "\n",
    "    classReport = []\n",
    "    for class_name in classes:\n",
    "        row = [report[class_name][metric] for metric in metrics]\n",
    "        classReport.append(row)\n",
    "    classReport = np.array(classReport)\n",
    "\n",
    "    sns.heatmap(classReport, annot=True, fmt='.3f', cmap='Blues', xticklabels=metrics, yticklabels=classes, vmin=0.0, vmax=1.0)\n",
    "    plt.title('Classification Report pt.1')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    ifications = list(report.keys())[-3:]\n",
    "    macro_avg = [report['macro avg'][metric] for metric in metrics]\n",
    "    weighted_avg = [report['weighted avg'][metric] for metric in metrics]\n",
    "    accuracy = report['accuracy']\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    ificationReport = np.array([macro_avg, weighted_avg])\n",
    "    sns.heatmap(ificationReport, annot=True, fmt='.3f', cmap='Greens', xticklabels=metrics, \n",
    "                yticklabels=['macro avg', 'weighted avg'], vmin=0.0, vmax=1.0)\n",
    "\n",
    "    print(\"Accuracy is: \", accuracy)\n",
    "    print(\"Precision score is: \", precision)\n",
    "\n",
    "    plt.title('Classification Report pt.2')\n",
    "    plt.show()\n",
    "\n",
    "def Fancy_Confusion_Matrix(y_true, y_pred):\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds', \n",
    "                xticklabels=['Predicted D or L', 'Predicted W'],\n",
    "                yticklabels=['Actually D or L','Actually W'])\n",
    "    print(conf_matrix)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report , precision_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt # to help in plotting results in a readable manner\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import RandomizedSearchCV , GridSearchCV\n",
    "#DNN\n",
    "import keras\n",
    "from keras import Sequential , layers\n",
    "from keras.layers import Dense , Dropout , Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV , RandomizedSearchCV\n",
    "from scipy.stats import randint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data:\n",
    "- load the csv file with the data\n",
    "- use head to show the first five column of the inital table before any modifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matches_initial = pd.read_csv(\"matches.csv\", index_col=0)\n",
    "matches_initial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing data with random forest before it is modified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_initial.index = range(matches_initial.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matches_initial = matches_initial.drop(\"Match Report\", axis=1)\n",
    "matches_initial = matches_initial.drop(\"Notes\", axis=1)\n",
    "matches_initial.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "matches_initial.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(38 * 20 * 5)\n",
    "matches_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matches_initial[\"Team\"].value_counts())\n",
    "4+4+4+4+4+4+4+4+4+5+5+5+5+5+5+5+6+6+6+7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_initial.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_initial[\"Date\"] = pd.to_datetime(matches_initial[\"Date\"])\n",
    "matches_initial.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_initial[\"Venue_Code\"] = matches_initial[\"Venue\"].astype(\"category\").cat.codes\n",
    "matches_initial[\"Opponent_Code\"] = matches_initial[\"Opponent\"].astype(\"category\").cat.codes\n",
    "matches_initial[\"Hour\"] = matches_initial[\"Time\"].str.replace(\":.+\",\"\", regex=True).astype(\"int\")\n",
    "matches_initial[\"Day_Code\"] = matches_initial[\"Date\"].dt.dayofweek\n",
    "matches_initial[\"Target\"] = (matches_initial[\"Result\"] == \"W\").astype(\"int\")\n",
    "matches_initial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random forest :\n",
    "To test how the accuracy changed after modifying the data we test it with a classifier before and after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=1000, min_samples_split=10, random_state=42)\n",
    "initial_train = matches_initial[matches_initial[\"Date\"] <= '2022-01-01']\n",
    "initial_test = matches_initial[matches_initial[\"Date\"] > '2022-01-01']\n",
    "initial_preds = [\"Venue_Code\", \"Opponent_Code\", \"Hour\", \"Day_Code\"]\n",
    "rf_clf.fit(initial_train[initial_preds], initial_train[\"Target\"])\n",
    "predictions_initial = rf_clf.predict(initial_test[initial_preds])\n",
    "accuracy = accuracy_score(initial_test[\"Target\"], predictions_initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_pred = pd.DataFrame(dict(Actual=initial_test[\"Target\"], Prediction=predictions_initial))\n",
    "print(y_true_pred)\n",
    "print(classification_report(y_true_pred[\"Actual\"], y_true_pred[\"Prediction\"]))\n",
    "Fancy_Class_Report(y_true_pred[\"Actual\"], y_true_pred[\"Prediction\"])\n",
    "Fancy_Confusion_Matrix(y_true_pred[\"Actual\"], y_true_pred[\"Prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modify the data:\n",
    "- form function adds more predictors by adding average of last 5 games\n",
    "- make_predicctions function takes in data and predictors and outputs confusion matrix and classificationn report and then gives dictionary with actual and predicted values\n",
    "- MssingDictionary finction normalises the team names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form(team, columns, new_columns):\n",
    "    team = team.sort_values(\"Date\")\n",
    "    form_stats = team[columns].rolling(5, closed='left').mean()\n",
    "    team[new_columns] = form_stats\n",
    "    team = team.dropna(subset=new_columns)\n",
    "    return team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_non_ann(data, predictors, clf):\n",
    "    train = data[data[\"Date\"] <= '2022-01-01']\n",
    "    test = data[data[\"Date\"] > '2022-01-01']\n",
    "    \n",
    "    clf.fit(train[predictors], train[\"Target\"])\n",
    "    predicts = clf.predict(test[predictors])\n",
    "    \n",
    "    \n",
    "    true_pred_dict = pd.DataFrame(dict(Actual=test[\"Target\"], Prediction=predicts, index=test.index))\n",
    "    Fancy_Confusion_Matrix(true_pred_dict['Actual'], true_pred_dict['Prediction'])\n",
    "    Fancy_Class_Report(true_pred_dict['Actual'], true_pred_dict['Prediction'])\n",
    "    print(classification_report(true_pred_dict['Actual'], true_pred_dict['Prediction']))\n",
    "    return true_pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingDictionary(dict):\n",
    "    __missing__ = lambda self, key: key\n",
    "\n",
    "mapping_vals = {\n",
    "    \"Brighton and Hove Albion\": \"Brighton\",\n",
    "    \"Manchester United\": \"Manchester Utd\",\n",
    "    \"Newcastle United\": \"Newcastle Utd\",\n",
    "    \"Tottenham Hotspur\": \"Tottenham\",\n",
    "    \"West Ham United\": \"West Ham\",\n",
    "    \"Wolverhampton Wanderers\": \"Wolves\",\n",
    "    \"Nottingham Forest\": \"Nott'ham Forest\",\n",
    "    \"West Bromwich Albion\": \"West Brom\",\n",
    "    \"Sheffield United\": \"Sheffield Utd\",\n",
    "    \"Huddersfield Town\": \"Huddersfield\"\n",
    "    \n",
    "}\n",
    "mapping_ = MissingDictionary(**mapping_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_col_names = [\"xG\", \"xGA\", \"Poss\", \"Dist\", \"FK\", \"PK\", \"PKatt\"]\n",
    "form_columns = [f\"{c}_form\" for c in f_col_names]\n",
    "matches = matches_initial.groupby(\"Team\").apply(lambda x: form(x, f_col_names, form_columns))\n",
    "matches = matches.droplevel(\"Team\")\n",
    "matches[\"Formation_Code\"] = matches[\"Formation\"].astype(\"category\").cat.codes\n",
    "print(matches.head())\n",
    "\n",
    "\n",
    "formation = [\"Formation_Code\"]\n",
    "predictors = initial_preds + form_columns + formation\n",
    "results_table =make_predictions_non_ann(matches, predictors,rf_clf)\n",
    "results_table = results_table.merge(matches[[\"Date\", \"Team\", \"Opponent\", \"Result\"]], left_index=True, right_index=True)\n",
    "results_table[\"Normalised_Team\"] = results_table[\"Team\"].map(mapping_)\n",
    "results_merged = results_table.merge(results_table, left_on=[\"Date\", \"Normalised_Team\"], right_on=[\"Date\", \"Opponent\"])\n",
    "print(results_merged[(results_merged[\"Prediction_x\"]==1) & (results_merged[\"Prediction_y\"]== 0)][\"Actual_x\"].value_counts())\n",
    "results_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = matches[matches[\"Date\"] <= '2022-01-01']\n",
    "test = matches[matches[\"Date\"] > '2022-01-01']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((results_table[\"Actual\"] == 1).sum())\n",
    "print((results_table[\"Actual\"] == 0).sum())\n",
    "print((results_table[\"Prediction\"] == 1).sum())\n",
    "print((results_table[\"Prediction\"] == 0).sum())\n",
    "results_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM model\n",
    "#### RBF kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the SVM model\n",
    "svm_model_rbf = SVC(random_state=0)\n",
    "\n",
    "results_table = make_predictions_non_ann(matches, predictors, svm_model_rbf)\n",
    "results_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create the SVM model\n",
    "svm_model = SVC(random_state=0,kernel='poly',C=10,degree=4)\n",
    "\n",
    "results_table = make_predictions_non_ann(matches, predictors, svm_model)\n",
    "results_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=len(predictors), activation='relu', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=42)))\n",
    "model.add(Dense(16, activation='relu', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=42)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=42)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(train[predictors], train[\"Target\"], epochs=1000, validation_split=0.2, callbacks=[early_stop])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(test[predictors], test[\"Target\"])\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy = model.evaluate(test[predictors],test[\"Target\"])\n",
    "print('loss:', loss, 'accuracy:', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict(test[predictors])\n",
    "\n",
    "y_pred = np.round(y_pred_prob)\n",
    "\n",
    "Fancy_Confusion_Matrix(test[\"Target\"], y_pred)\n",
    "Fancy_Class_Report(test[\"Target\"], y_pred)\n",
    "print(classification_report(test[\"Target\"], y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempting to add extra layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=len(predictors), activation='relu', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=42)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=42)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='relu', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=42)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(train[predictors], train[\"Target\"], epochs=1000, validation_split=0.2, callbacks=[early_stop])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(test[predictors], test[\"Target\"])\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy = model.evaluate(test[predictors],test[\"Target\"])\n",
    "print('loss:', loss, 'accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict(test[predictors])\n",
    "\n",
    "y_pred = np.round(y_pred_prob)\n",
    "\n",
    "Fancy_Confusion_Matrix(test[\"Target\"], y_pred)\n",
    "Fancy_Class_Report(test[\"Target\"], y_pred)\n",
    "print(classification_report(test[\"Target\"], y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding the extra layers the DNN is performing worse which might be due to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned deep neural network:\n",
    "\n",
    "\n",
    "#### Create network and wrap it using kerasClassifier to perform gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def create_model(layers, activation):\n",
    "    model = Sequential()\n",
    "    for i, nodes in enumerate(layers):\n",
    "        if i==0:\n",
    "            model.add(Dense(nodes, input_dim=len(predictors)))\n",
    "            model.add(Activation(activation=activation))\n",
    "        else:\n",
    "            model.add(Dense(nodes))\n",
    "            model.add(Activation(activation=activation))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = KerasClassifier(build_fn=create_model,verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters to tune\n",
    "'''layers = [[32], [16, 8], [64, 32, 16]]\n",
    "activations = ['sigmoid', 'relu']\n",
    "# Define the grid search parameters\n",
    "param_grid = dict(layers=layers, activation=activations, batch_size=[16, 32], epochs=[100, 500])\n",
    "# Create a GridSearchCV object\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_result = grid.fit(train[predictors], train['Target'])\n",
    "\n",
    "# Print the best score and parameters\n",
    "print('Best Score:', grid_result.best_score_)\n",
    "print('Best Params:', grid_result.best_params_)\n",
    "\n",
    "# Calculate the accuracy on the test data\n",
    "accuracy = grid.score(test[predictors], test['Target'])\n",
    "print('Test Accuracy:', accuracy)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it was a fail as gridsearch is computationaly expensive for large datasets, it was taking an incredibly long time to  find the best params "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create network with more hyperparameter options to use Randomsearch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the fact that GridSearchcv takes a really long time , RandomizedSearchCV had to be used to find the best values faster , it may not be as accurate but we can add more options for it to choose from and it will take less time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def create_model(hidden_layers=5, units=16, dropout=0.1, optimizer='adam', activation='relu', epochs=100):\n",
    "    model = Sequential()\n",
    "    for i in range(hidden_layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(units=units, input_dim=len(predictors), activation=activation))\n",
    "            model.add(Dropout(dropout))\n",
    "        else:\n",
    "            model.add(Dense(units=units, activation=activation))\n",
    "            model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    history = model.fit(train[predictors], train['Target'], epochs=epochs, validation_split=0.2)\n",
    "    return model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = KerasClassifier(build_fn=create_model, verbose=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters to tune\n",
    "'''param_dist = {\n",
    "    'hidden_layers': randint(1, 5),\n",
    "    'units': randint(8, 64),\n",
    "    'dropout': [0.1, 0.2, 0.3],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "    'epochs': [100, 200, 300]}\n",
    "\n",
    "\n",
    "\n",
    "# Create a RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the RandomizedSearchCV object to the training data\n",
    "random_search.fit(train[predictors], train['Target'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best score and parameters\n",
    "#print('Best Score:', random_search.best_score_)\n",
    "#print('Best Params:', random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the accuracy on the test data\n",
    "#accuracy = random_search.score(test[predictors], test['Target'])\n",
    "#print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the fact that random search does not consider all possible combinations of hyperparameters , it was not successful at increasing the dnn's accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing performance\n",
    "### Classification report comparison: \n",
    "| model type | precision | recall | f1-score| support|\n",
    "| :-: | :-: | :-: | :-:| :-: |\n",
    "|Random forest classifier| 0.60 | 0.62 | 0.60 | 1052|\n",
    "|Random forest classifier modified| 0.64 | 0.65 | 0.62 | 1047|\n",
    "|SVM model with rbf kernel | 0.64 | 0.64 | 0.52 | 1047|\n",
    "|SVM model with poly kernel |0.64|0.64|0.57|1047|\n",
    "|deep neural network |0.62|0.63|0.61|1047|\n",
    "|deep neural network with extra layers|0.62|0.61|0.48|1047|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy comparison: \n",
    "- Random forest classifier:61.6%\n",
    "- Random forest classifier modified:64.8%\n",
    "- SVM model with rbf kernel :63.7%\n",
    "- SVM model with poly kernel : 63.8%\n",
    "- deep neural network :63.6%\n",
    "- deep neural network with extra layers:61.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen from the comparison when adding the preprocessed data to the random forest classifier it is the best performing with the highest accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine Learning",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
